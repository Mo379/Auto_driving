{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad417aee",
   "metadata": {},
   "source": [
    "# Importing pakages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e81ec912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pickle\n",
    "import time\n",
    "from src.datahandle import *\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cffa89",
   "metadata": {},
   "source": [
    "# Setting up hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5713f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper\n",
    "directory = '../../extras/data/A_training_given/training_data/'\n",
    "training_folder = 'training_data'\n",
    "training_labels_file= 'training_norm.csv'\n",
    "#\n",
    "collected_directory = '../../extras/data/Z_collected_raw/' \n",
    "#\n",
    "quiz_directory = '../../extras/data/C_testing_given/test_data/'\n",
    "quiz_training_folder = 'test_data'\n",
    "#configurations\n",
    "conf_tracking = 0\n",
    "seed = 0\n",
    "data_shape = 'original'\n",
    "parameter_init_scale = 0.01\n",
    "split= 0.8\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b297e6b",
   "metadata": {},
   "source": [
    "# Setting up DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8112d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloading object\n",
    "training_object= DataLoader(\n",
    "        directory,\n",
    "        training_folder,\n",
    "        training_labels_file\n",
    "        )\n",
    "collected_object = DataLoader(\n",
    "        collected_directory        \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee6f57",
   "metadata": {},
   "source": [
    "# Loading data information (not loading images yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4f0219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_train, collected_test = collected_object.LoadCollectedData_info(\n",
    "        split=split,\n",
    "        batch_size=batch_size #for tf this batch size remains as 1\n",
    "        )\n",
    "#train test split\n",
    "train,test = training_object.LoadModelData_info(\n",
    "        split = split, \n",
    "        batch_size =batch_size)#for tf this batch size remains as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70c24fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['../../extras/data/Z_collected_raw/1648218352630_100_35.png',\n",
       "        '1648218352630', '0.714', '0.868'], dtype='<U58'),\n",
       " array([['../../extras/data/A_training_given/training_data/training_data/4954.png',\n",
       "         '4954', '0.625', '0.0'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/5492.png',\n",
       "         '5492', '0.5', '0.901'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/3785.png',\n",
       "         '3785', '0.75', '0.921'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/6825.png',\n",
       "         '6825', '0.6875', '0.988'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/8808.png',\n",
       "         '8808', '0.6875', '0.97'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/406.png',\n",
       "         '406', '0.8125', '0.999'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/1192.png',\n",
       "         '1192', '0.625', '0.922'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/12913.png',\n",
       "         '12913', '0.625', '0.997'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/1179.png',\n",
       "         '1179', '0.625', '0.93'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/6616.png',\n",
       "         '6616', '0.3125', '0.905'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/9525.png',\n",
       "         '9525', '0.5', '0.87'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/7508.png',\n",
       "         '7508', '0.4375', '0.983'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/12720.png',\n",
       "         '12720', '0.5625', '0.934'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/11229.png',\n",
       "         '11229', '0.5625', '0.893'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/4001.png',\n",
       "         '4001', '0.8125', '0.944'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/2470.png',\n",
       "         '2470', '0.5625', '0.951'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/10137.png',\n",
       "         '10137', '0.5625', '0.053'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/2316.png',\n",
       "         '2316', '0.625', '0.941'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/10651.png',\n",
       "         '10651', '0.3125', '0.037'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/5479.png',\n",
       "         '5479', '0.625', '0.855'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/4767.png',\n",
       "         '4767', '0.5625', '0.053'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/10889.png',\n",
       "         '10889', '0.8125', '0.95'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/3008.png',\n",
       "         '3008', '0.875', '0.974'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/12046.png',\n",
       "         '12046', '0.4375', '0.982'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/6170.png',\n",
       "         '6170', '0.8125', '0.901'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/9243.png',\n",
       "         '9243', '0.8125', '0.877'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/13358.png',\n",
       "         '13358', '0.75', '0.033'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/12052.png',\n",
       "         '12052', '0.75', '0.056'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/8149.png',\n",
       "         '8149', '0.5625', '0.898'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/6164.png',\n",
       "         '6164', '0.8125', '0.911'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/9257.png',\n",
       "         '9257', '0.5', '0.997'],\n",
       "        ['../../extras/data/A_training_given/training_data/training_data/2302.png',\n",
       "         '2302', '0.8125', '0.908']], dtype='<U72'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected_train[0][1],train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9801be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8016749e",
   "metadata": {},
   "source": [
    "# Stacking collected and kaggle given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b7b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_XY = np.vstack((train,collected_train))\n",
    "test_XY= np.vstack((test[0],collected_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f77456a",
   "metadata": {},
   "source": [
    "# Checking DataLoading functionality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "033ea7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 320, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    test_batch_X,test_batch_Y = training_object.Load_batch(train_XY[i], data_shape=data_shape)\n",
    "image_shape = test_batch_X.shape[1:]\n",
    "image_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0d6c8",
   "metadata": {},
   "source": [
    "# Setting up data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "453a83c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "        layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "        layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "        #layers.experimental.preprocessing.RandomContrast(0.2),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb5ee0",
   "metadata": {},
   "source": [
    "# Setting up preprocessing for MobilenetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06442fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4832273",
   "metadata": {},
   "source": [
    "# Transfer Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e56a9",
   "metadata": {},
   "source": [
    "## to do\n",
    "- [x] Seperate TF_cnn from TF_transfer\n",
    "- [x] Remove rescaling in datahandling step (Now done by the preprocess layer)\n",
    "- [x] Correctly rescale collected images labels (speed and angle)\n",
    "- [x] Soften output\n",
    "- [ ] Train the transfer learning model\n",
    "- [ ] Tune the transfer learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25ec2cd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "ImageNetBase = tf.keras.applications.MobileNetV2(input_shape=image_shape,include_top=False,weights='imagenet')\n",
    "ImageNetBase.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "663531d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_batch = ImageNetBase(test_batch_X)\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "output = global_average_layer(feature_batch)\n",
    "inference_layer = tf.keras.layers.Dense(2)\n",
    "ouput2 = inference_layer(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1808ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "dense1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "dense2 = tf.keras.layers.Dense(128, activation='relu')\n",
    "inference_layer = tf.keras.layers.Dense(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97b0d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=image_shape)\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = ImageNetBase(x)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = dense1(x)\n",
    "x = dense2(x)\n",
    "outputs = inference_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38f2c539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x157f44f70>,\n",
       " <keras.engine.sequential.Sequential at 0x157f0b7f0>,\n",
       " <keras.layers.core.tf_op_layer.TFOpLambda at 0x157a50400>,\n",
       " <keras.layers.core.tf_op_layer.TFOpLambda at 0x157f21e80>,\n",
       " <keras.engine.functional.Functional at 0x157e1f880>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x157f349d0>,\n",
       " <keras.layers.core.dropout.Dropout at 0x157f3efa0>,\n",
       " <keras.layers.core.dense.Dense at 0x157d7ca00>,\n",
       " <keras.layers.core.dense.Dense at 0x157aa39a0>,\n",
       " <keras.layers.core.dense.Dense at 0x157f3e490>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de7fd5",
   "metadata": {},
   "source": [
    "# Weights and Biases configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6be93b54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wandb tracking\n",
    "if conf_tracking:\n",
    "    config = {\n",
    "     \"model_type\" : 'Transfer Learning with MobileNetV2',\n",
    "     \"model_shape\" : str(model.layers),\n",
    "     \"learning_rate\": lr,\n",
    "     \"data_split\": split,\n",
    "     \"batch_size\": batch_size,\n",
    "     \"epochs\": n_epochs,\n",
    "    }\n",
    "    run = wandb.init(project=\"Autonomous-driving\", entity=\"mo379\",config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfe424f",
   "metadata": {},
   "source": [
    "# Training Loop (With wandb logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec08a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'pkls/tf_cnn_augmented.pkl'\n",
    "#model = tf.keras.models.load_model(model_path, custom_objects=None, compile=True, options=None)\n",
    "test_batches = test_XY[0:2930].reshape(10,-1,4)\n",
    "for _ in range(n_epochs):\n",
    "    for i in range(1):\n",
    "        X,Y = training_object.Load_batch(train_XY[i], data_shape=data_shape)\n",
    "        train_metrics = model.train_on_batch(X,Y, return_dict=True) \n",
    "\n",
    "        X,Y = training_object.Load_batch(test_batches[np.random.randint(0,9)], data_shape=data_shape)\n",
    "        test_metrics = model.test_on_batch(\n",
    "            X, Y, sample_weight=None, reset_metrics=True, return_dict=True\n",
    "        )\n",
    "        \n",
    "        if conf_tracking==1:\n",
    "            wandb.log({\"test_loss\": test_metrics['loss']})\n",
    "            wandb.log({\"batch_loss\": train_metrics['loss']})\n",
    "#tf.keras.models.save_model(\n",
    "#    model,\n",
    "#    model_path,\n",
    "#    overwrite=False,\n",
    "#    include_optimizer=True,\n",
    "#    save_format=None,\n",
    "#    signatures=None,\n",
    "#    options=None,\n",
    "#    save_traces=True\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def0ecb",
   "metadata": {},
   "source": [
    "# Loading the Quiz data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_object= DataLoader(\n",
    "    quiz_directory,\n",
    "    quiz_training_folder,\n",
    ")\n",
    "quiz_train = quiz_object.LoadQuizData_info()\n",
    "X,image_order = quiz_object.Load_batch_quiz(quiz_train,data_shape=data_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ab7204",
   "metadata": {},
   "source": [
    "# Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a41ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prds = model.predict(X)\n",
    "final_prd = np.column_stack((image_order,prds))\n",
    "final_ordered = final_prd[final_prd[:, 0].argsort()]\n",
    "df = pd.DataFrame(final_ordered, columns = ['image_id','angle','speed'])\n",
    "df = df.astype({'image_id': 'int32'})\n",
    "df.to_csv('submission.csv', index=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b7d1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlis_car",
   "language": "python",
   "name": "mlis_car"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
